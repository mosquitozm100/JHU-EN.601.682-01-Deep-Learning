{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Learning: Lab 2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5P78cfEReAm",
        "colab_type": "text"
      },
      "source": [
        "Download text file. It's the first chuck of the Gigawords dataset (https://catalog.ldc.upenn.edu/LDC2003T05)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHbQ8GpLdVTh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://www.dropbox.com/s/b6mmt7w115onvfp/text.gz && gzip -d text.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HJo1t4SY9KX",
        "colab_type": "text"
      },
      "source": [
        "sample of the text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQmmDVZxLxgi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!head text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_gw0efYDdZ2",
        "colab_type": "text"
      },
      "source": [
        "# Word2vec Demo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCkLEiDS5_ij",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4b3d32fa-b1f8-40ba-866a-a98a84b5e52c"
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow is already loaded. Please restart the runtime to change versions.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GBYVzIgA8yv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "\n",
        "from tqdm import tqdm\n",
        "from gensim.models import Word2Vec\n",
        "from typing import List\n",
        "\n",
        "# workaround a bug\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import tensorflow as tf\n",
        "import tensorboard as tb\n",
        "tf.io.gfile = tb.compat.tensorflow_stub.io.gfile"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09BGfBqRA81X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the text file\n",
        "sents = []\n",
        "with open('text') as fp:\n",
        "    for line in fp:\n",
        "        sents.append([word.lower() for word in line.strip().split()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRbUrpPOMGew",
        "colab_type": "text"
      },
      "source": [
        "Train a CBOW model with embeddings size 50, window size 5. We ignore any word with count smaller than 50."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srQSSBOdMFNr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Word2Vec(sents, size=50, window=5, min_count=50, sg=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmKTZxMCAQRd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Embeddings:\n",
        "    def __init__(self, embeddings: np.ndarray, vocabulary: List[str]):\n",
        "        assert len(embeddings) == len(vocabulary)\n",
        "        self.embeddings = embeddings\n",
        "        self.word2id = {w:i for i, w in enumerate(vocabulary)}\n",
        "        self.id2word = vocabulary\n",
        "\n",
        "    def vector(self, word: str):\n",
        "        assert word in self.word2id, f'{word} is not in the vocabulary'\n",
        "        return self.embeddings[self.word2id[word]]\n",
        "\n",
        "    def nearest_neighbor(self, query: np.ndarray, topk=10):\n",
        "        '''\n",
        "        given a query, print the topk nearest words vectors measured by cosine similarity\n",
        "        cos_sim(v1, v2) = dot_product(v1, v2) / (||v1|| * ||v2||)\n",
        "        '''\n",
        "        ############\n",
        "        ### TODO ###\n",
        "        ############\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def analogy(self, w1: str, w2: str, w3: str, topk=10):\n",
        "        v1 = self.vector(w1)\n",
        "        v2 = self.vector(w2)\n",
        "        v3 = self.vector(w3)\n",
        "        query = v1 - v2 + v3\n",
        "        self.nearest_neighbor(query, topk=topk)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVc1tqcd_JEo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "official_embedding = Embeddings(model.wv.vectors, model.wv.index2word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUENdCZaMW3_",
        "colab_type": "text"
      },
      "source": [
        "Find the most similar word under cosine similarity.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uI20GGg6A84K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word = 'obama' #@param {type: \"string\"}\n",
        "word_embedding = official_embedding.vector(word)\n",
        "official_embedding.nearest_neighbor(word_embedding, topk=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zm8bmyXaM86E",
        "colab_type": "text"
      },
      "source": [
        "Find the word $x$ such that\n",
        "$$v_{w_1} - v_{w_2} \\approx v_{x} - v_{w_3}$$\n",
        "$$v_{x} \\approx  v_{w_1} - v_{w_2} + v_{w_3}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JK2lVcIzA87C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "w1 = 'king' #@param {type: \"string\"}\n",
        "w2 = 'men' #@param {type: \"string\"}\n",
        "w3 = 'women' #@param {type: \"string\"}\n",
        "official_embedding.analogy(w1, w2, w3, topk=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADLa2z2ZNQlg",
        "colab_type": "text"
      },
      "source": [
        "We can also visualize the whole embeddings space using TensorBoard."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOiKnaUmA89S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "writer = SummaryWriter('runs/offical')\n",
        "writer.add_embedding(model.wv.vectors, model.wv.index2word)\n",
        "writer.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6mDiY4bD7x3",
        "colab_type": "text"
      },
      "source": [
        "Click the menu -> projector to find the embeddings visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJR4j6tHA9Ae",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs/offical"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSkSlws2CJPd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwHrweZ5DpnZ",
        "colab_type": "text"
      },
      "source": [
        "# Word2vec from scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7t8og_diEj-N",
        "colab_type": "text"
      },
      "source": [
        "Hyperparameters\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6Y1KwaK0yX7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# only keep word with count no less than MIN_COUNT\n",
        "MIN_COUNT = 50      # @param {type: \"number\"}\n",
        "# only keep DATA_SIZE number of examples\n",
        "DATA_SIZE = 1000000 # @param {type: \"number\"}\n",
        "# maximum distance between center word and words in the window\n",
        "WINDOW_SIZE = 5     # @param {type: \"number\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pu6WFjEdQ7Hg",
        "colab_type": "text"
      },
      "source": [
        "## Setup the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmKSdgIueA80",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "UNK = '<UNK>'\n",
        "UNK_ID = 0\n",
        "\n",
        "class Vocabulary:\n",
        "    def __init__(self, filepath, min_count=3):\n",
        "        vocab = Counter()\n",
        "        with open(filepath) as fp:\n",
        "            for line in fp:\n",
        "                words = [word.lower() for word in line.strip().split()]\n",
        "                vocab.update(words)\n",
        "        self.word2id = {UNK: UNK_ID}\n",
        "        self.id2word = [UNK]\n",
        "        for word, count in vocab.most_common():\n",
        "            if count >= min_count:\n",
        "                self.word2id[word] = len(self.word2id)\n",
        "                self.id2word.append(word)\n",
        "        print(f'vocabulary size = {self.vocab_size()}')\n",
        "\n",
        "    def index_text(self, filepath):\n",
        "        assert self.word2id is not None\n",
        "        assert self.id2word is not None\n",
        "        text = []\n",
        "        with open(filepath) as fp:\n",
        "            for line in fp:\n",
        "                for word in line.strip().split():\n",
        "                    word = word.lower()\n",
        "                    if word not in self.word2id:\n",
        "                        word = UNK\n",
        "                    text.append(self.word2id[word])\n",
        "        text = np.array(text)\n",
        "        print(f'OOV ratio = {sum(text == 0) / len(text) * 100:.2f}%')\n",
        "        return text\n",
        "\n",
        "    def vocab_size(self):\n",
        "        return len(self.word2id)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdkTDFWRngg3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_dataset(text, window_size=5, data_size=None):\n",
        "    '''\n",
        "    window_size: context of one size\n",
        "    '''\n",
        "    x, y = [], []\n",
        "    for i in range(window_size,len(text) - window_size):\n",
        "        left_window = text[i-window_size:i]\n",
        "        right_window = text[i+1:i+1+window_size]\n",
        "        window = np.concatenate((left_window, right_window))\n",
        "        # ignore anything with UNK\n",
        "        if text[i] == UNK_ID:\n",
        "            continue\n",
        "        if (window == UNK_ID).sum() > 2:\n",
        "            continue\n",
        "        x.append(window)\n",
        "        y.append(text[i])\n",
        "    x, y = np.stack(x), np.stack(y)\n",
        "\n",
        "    # shuffle data\n",
        "    idx = np.arange(len(x))\n",
        "    np.random.shuffle(idx)\n",
        "    x, y = x[idx], y[idx]\n",
        "\n",
        "    if data_size is not None:\n",
        "        x, y = x[:data_size], y[:data_size]\n",
        "\n",
        "    # split data\n",
        "    train_size = int(len(x) * 0.9)\n",
        "    train_x, train_y = x[:train_size], y[:train_size]\n",
        "    dev_x, dev_y     = x[train_size:], y[train_size:]\n",
        "    return (train_x, train_y), (dev_x, dev_y)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nHmRYPohTmy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = Vocabulary('text', min_count=MIN_COUNT)\n",
        "text = vocab.index_text('text')\n",
        "print(f'number of tokens = {len(text)}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dzru0-UdhTwb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(train_x, train_y), (dev_x, dev_y) = build_dataset(text, window_size=WINDOW_SIZE, data_size=DATA_SIZE)\n",
        "print('training size')\n",
        "print(train_x.shape, train_y.shape)\n",
        "print('dev size')\n",
        "print(dev_x.shape, dev_y.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbHkR5FVRGCO",
        "colab_type": "text"
      },
      "source": [
        "## Main Model: Continous Bag-Of-Words (CBOW)\n",
        "\n",
        "Initialize embeddings matrix and each row represents embeddings of a word, let the input embeddings as $V$ and output embeddings as $U$. Output embeddings is used for softmax."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBMbUAFek7YX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EMBED_SIZE = 50 # 20\n",
        "\n",
        "input_embed = np.random.randn(vocab.vocab_size(), EMBED_SIZE) * EMBED_SIZE ** -0.5\n",
        "output_embed = np.random.randn(vocab.vocab_size(), EMBED_SIZE) * EMBED_SIZE ** -0.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xurKTRDih54X",
        "colab_type": "text"
      },
      "source": [
        "### CBOW forward pass\n",
        "\n",
        "Assuming we have a slice of words in a sentence $w_{-2}$, $w_{-1}$, $w_{c}$, $w_{1}$, $w_{2}$. We want to predict $w_{c}$ with $w_{-2}$, $w_{-1}$, $w_{1}$, $w_{2}$. Let the vector of a word $w$ from embeddings $V$ and $U$ be $v_w$ and $u_w$, respectively. Let the vocabulary be $\\mathcal{V}$.\n",
        "\n",
        "$$v_{bow} = v_{w_{-2}} + v_{w_{-1}}+ v_{w_{1}}+ v_{w_{2}}$$\n",
        "$$p(w_c|v_{bow}) = \\frac {\\exp(v_{bow} \\cdot u_{w_c})} {\\sum_{w_j\\in\\mathcal{V}} \\exp(v_{bow} \\cdot u_{w_j})}$$\n",
        "$$\\mathcal{L}_{w_c} = - \\log p(w_c|v_{bow})$$\n",
        "\n",
        "In practise, we take the average over $\\mathcal{L}_{w_c}$ of a batch of example, as loss of the batch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkElBmoFCCCx",
        "colab_type": "text"
      },
      "source": [
        "### CBOW backward pass\n",
        "\n",
        "For each sample of $w_{-2}$, $w_{-1}$, $w_{c}$, $w_{1}$, $w_{2}$, we need the work out the gradient of $- \\log p(w_c|v_{bow})$ with respect to $u_{w_c}$, $u_w$ where $w \\neq w_c$ and $v$ where $v \\in \\{v_{w_{-2}}, v_{w_{-1}}, v_{w_{1}}, v_{w_{2}}\\}$.\n",
        "\n",
        "Try to work it out\n",
        "\n",
        "$$\\frac{\\partial}{\\partial u_{w_c}} - \\log p(w_c|v_{bow}) = \\frac{\\partial}{\\partial u_{w_c}} - \\log \\frac {\\exp(v_{bow} \\cdot u_{w_c})} {\\sum_{w_j\\in\\mathcal{V}} \\exp(v_{bow} \\cdot u_{w_j})}$$\n",
        "\n",
        "$$\\frac{\\partial}{\\partial u_w} - \\log p(w_c|v_{bow}) = \\frac{\\partial}{\\partial u_w} - \\log \\frac {\\exp(v_{bow} \\cdot u_{w_c})} {\\sum_{w_j\\in\\mathcal{V}} \\exp(v_{bow} \\cdot u_{w_j})}$$\n",
        "\n",
        "$$ \\frac{\\partial}{\\partial v} - \\log p(w_c|v_{bow}) = \\frac{\\partial}{\\partial v} - \\log \\frac {\\exp(v_{bow} \\cdot u_{w_c})} {\\sum_{w_j\\in\\mathcal{V}} \\exp(v_{bow} \\cdot u_{w_j})}$$\n",
        "\n",
        "Hint 1: \n",
        "\n",
        "$\\frac{\\partial}{\\partial v} u \\cdot v = u$\n",
        "\n",
        "$\\frac{\\partial}{\\partial u} u \\cdot v = v$\n",
        "\n",
        "$\\frac{\\partial}{\\partial x} \\exp f(x) = \\exp \\frac{\\partial}{\\partial x} f(x)$\n",
        "\n",
        "$\\frac{\\partial}{\\partial x} \\log f(x) = \\frac {1} {f(x)} \\frac{\\partial}{\\partial x} f(x)$\n",
        "\n",
        "\n",
        "Hint 2: Additionally, we need to consider the average part when computing the gradient\n",
        "\n",
        "\n",
        "Hint 3: Let $X = \\{x_1, \\cdots, x_N\\}$, when $x_i$ is to small, it could cause numerical instability when taking log. As a result, we can use the following tricks to compute sotfmax and log jointly. Let $b = \\max(X)$,\n",
        "\n",
        "\\begin{align}\n",
        "\\log p(x_i|X)\n",
        "&= \\log \\frac {\\exp(x_i)} {\\sum_j \\exp(x_j)} \\\\\n",
        "&= \\log \\frac {\\exp(x_i)/\\exp(b)} {\\sum_j \\exp(x_j)/\\exp(b)} \\\\\n",
        "&= \\log \\frac {\\exp(x_i-b)} {\\sum_j \\exp(x_j-b)} \\\\\n",
        "&= \\log \\exp(x_i-b) - \\log\\sum^N_1 \\exp(x_j-b)\\\\\n",
        "&= x_i - (b + \\log\\sum^N_1 \\exp(x_j - b))\\\\\n",
        "\\end{align}\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQH5fnuAizqm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# learning rate\n",
        "LR = 1      # @param {type: \"number\"}\n",
        "\n",
        "# batch size\n",
        "BS = 50     # @param {type: \"number\"}\n",
        "\n",
        "step = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mguSsGWjSYX1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward_and_backward(batch_x, batch_y, input_embed, output_embed):\n",
        "    # input_embed_grad: [vocab_size, embedding_size]\n",
        "    input_embed_grad = np.zeros_like(input_embed)\n",
        "    # output_embed_grad: [vocab_size, embedding_size]\n",
        "    output_embed_grad = np.zeros_like(output_embed)\n",
        "    # loss: float\n",
        "    loss = 0\n",
        "\n",
        "    ############\n",
        "    ### TODO ###\n",
        "    ############\n",
        "\n",
        "    raise NotImplementedError\n",
        "\n",
        "    ### Forward Pass\n",
        "    # window_embed: [BS, window_size *2, embedding_size]\n",
        "    # bow_embed:    [BS, embedding_size]\n",
        "    # logits:       [BS, vocab_size]\n",
        "    # logprobs:     [BS, vocab_size]\n",
        "\n",
        "    ### Backward Pass\n",
        "    # probs:             [BS, vocab_size]\n",
        "    # output_embed_grad: [vocab_size, embedding_size]\n",
        "    # bow_embed_grad:    [BS, embedding_size]\n",
        "    # window_embed_grad: [BS, window_size *2, embedding_size]\n",
        "    # batch_x:           [BS, window_size *2]\n",
        "    # input_embed_grad:  [vocab_size, embedding_size]\n",
        "\n",
        "    return loss, input_embed_grad, output_embed_grad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2Fi9XROTj3l",
        "colab_type": "text"
      },
      "source": [
        "Make sure we get the gradient right"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODMwerA3SYiP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# use a prime number for batch size would make debugging easier\n",
        "bs = 7\n",
        "batch_x = train_x[:bs]\n",
        "batch_y = train_y[:bs]\n",
        "loss, input_embed_grad, output_embed_grad = forward_and_backward(batch_x, batch_y, input_embed, output_embed)\n",
        "\n",
        "_input_embed = torch.tensor(input_embed, requires_grad=True)\n",
        "_output_embed = torch.tensor(output_embed, requires_grad=True)\n",
        "_batch_x = torch.tensor(batch_x, requires_grad=False)\n",
        "_batch_y = torch.tensor(batch_y, requires_grad=False)\n",
        "_window_embed = F.embedding(_batch_x, _input_embed)\n",
        "_bow_embed = _window_embed.sum(dim=1)\n",
        "_logits = F.linear(_bow_embed, _output_embed)\n",
        "_logprobs = F.log_softmax(_logits, dim=-1)\n",
        "_loss = -_logprobs[torch.arange(len(_batch_y)), _batch_y].mean()\n",
        "_loss.backward()\n",
        "\n",
        "assert np.isclose(loss, _loss.item())\n",
        "assert np.allclose(_input_embed.grad.numpy(), input_embed_grad)\n",
        "assert np.allclose(_output_embed.grad.numpy(), output_embed_grad)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIkPRpp_TXxj",
        "colab_type": "text"
      },
      "source": [
        "Training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSGYhqNVuJxx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "running_loss = []\n",
        "for i in range(0, len(train_x), BS):\n",
        "    batch_x = train_x[i:i+BS]\n",
        "    batch_y = train_y[i:i+BS]\n",
        "\n",
        "    loss, input_embed_grad, output_embed_grad = forward_and_backward(batch_x, batch_y, input_embed, output_embed)\n",
        "    running_loss.append(loss)\n",
        "\n",
        "    # gradient updata\n",
        "    current_lr = LR * 0.995 ** (step // 100)\n",
        "    input_embed -= current_lr * input_embed_grad\n",
        "    output_embed -= current_lr * output_embed_grad\n",
        "\n",
        "    if step % 100 == 0:\n",
        "        print(f'step = {step}, LR = {current_lr:.3f}, running loss = {np.mean(running_loss):.3f} +- {np.std(running_loss):.3f}')\n",
        "        running_loss = []\n",
        "\n",
        "    step += 1\n",
        "\n",
        "    if step > 5000:\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYZldoPYCdJB",
        "colab_type": "text"
      },
      "source": [
        "# Inspect Trained Embeddings\n",
        "\n",
        "Now we can inspect the embedding space. First, we combine input and output embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCgmkYKEpiRI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_embed = (input_embed + output_embed) / 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SI9hhWPmNhXn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "our_embedding = Embeddings(final_embed, vocab.id2word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jN5RQZMLNhbU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word = 'obama' #@param {type: \"string\"}\n",
        "word_embedding = our_embedding.vector(word)\n",
        "our_embedding.nearest_neighbor(word_embedding, topk=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBhYFkmAN06L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "w1 = 'king' #@param {type: \"string\"}\n",
        "w2 = 'men' #@param {type: \"string\"}\n",
        "w3 = 'women' #@param {type: \"string\"}\n",
        "our_embedding.analogy(w1, w2, w3, topk=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_knjFsJuft7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzCRfgYZyPjz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "writer = SummaryWriter('runs/diy')\n",
        "writer.add_embedding(final_embed, vocab.id2word)\n",
        "writer.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6mz34OQ068Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs/diy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yq5jfYRNM2Bt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0yBEDqAM2E_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAz-f46WM25t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDMl1hjVM22-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlFZDvBMM209",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNMWLt6BM2zA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClEgln5jM2wd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FansS43qM2tk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9VjK5f9M2rP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxlIRQcRM2pA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gT7rWTepM2mq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7z_I4vvM2j6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBpf6YRSM2gz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXu19A9BM2ed",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnbwS6TWM2cb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npwkiPN1M2aR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cY5kEPIJM2YT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0G8yOyunM2WN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0g1E7oQM2UA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOg6HaH7M2Sa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CGHOjzRM2Pg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvmRcf1HM2Mj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPT7tQD6M2Ko",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZF3eP1AM2IF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hGqBnxGM1-F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJKS3aIxM3we",
        "colab_type": "text"
      },
      "source": [
        "Spoiler Alert: solution to the gradient.\n",
        "\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upnfWkr8O0a6",
        "colab_type": "text"
      },
      "source": [
        "\\begin{align}\n",
        "\\frac{\\partial}{\\partial u_{w_c}} - \\log p(w_c|v_{bow})\n",
        "&= \\frac{\\partial}{\\partial u_{w_c}} - \\log \\frac {\\exp(v_{bow} \\cdot u_{w_c})} {\\sum_{w_j\\in\\mathcal{V}} \\exp(v_{bow} \\cdot u_{w_j})} \\\\\n",
        "&= \\frac{\\partial}{\\partial u_{w_c}} - \\log  \\exp(v_{bow} \\cdot u_{w_c}) + \\log\\sum_{w_j\\in\\mathcal{V}} \\exp(v_{bow} \\cdot u_{w_j}) \\\\\n",
        "&= - \\frac{\\partial}{\\partial u_{w_c}} v_{bow} \\cdot u_{w_c} + \\frac{\\partial}{\\partial u_{w_c}} \\log\\sum_{w_j\\in\\mathcal{V}} \\exp(v_{bow} \\cdot u_{w_j}) \\\\\n",
        "&= - v_{bow} +  \\frac {1} {\\sum_{w_j\\in\\mathcal{V}} \\exp(v_{bow} \\cdot u_{w_j})}  \\frac{\\partial}{\\partial u_{w_c}} \\sum_{w_j\\in\\mathcal{V}} \\exp(v_{bow} \\cdot u_{w_j}) \\\\\n",
        "&= - v_{bow} +  \\frac {1} {\\sum_{w_j\\in\\mathcal{V}} \\exp(v_{bow} \\cdot u_{w_j})}  \\frac{\\partial}{\\partial u_{w_c}} \\exp(v_{bow} \\cdot u_{w_c}) \\\\\n",
        "&= - v_{bow} +  \\frac {\\exp(v_{bow} \\cdot u_{w_c}) } {\\sum_{w_j\\in\\mathcal{V}} \\exp(v_{bow} \\cdot u_{w_j})}  \\frac{\\partial}{\\partial u_{w_c}} v_{bow} \\cdot u_{w_c} \\\\\n",
        "&= - v_{bow} +  p(w_c|v_{bow}) v_{bow} \\\\\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{\\partial}{\\partial u_w} - \\log p(w_c|v_{bow})\n",
        "&= \\frac{\\partial}{\\partial u_w} - \\log \\frac {\\exp(v_{bow} \\cdot u_{w_c})} {\\sum_{w_j\\in\\mathcal{V}} \\exp(v_{bow} \\cdot u_{w_j})} \\\\\n",
        "&= \\frac{\\partial}{\\partial u_w} - \\log  \\exp(v_{bow} \\cdot u_{w_c}) + \\log\\sum_{w_j\\in\\mathcal{V}} \\exp(v_{bow} \\cdot u_{w_j}) \\\\\n",
        "&= - \\frac{\\partial}{\\partial u_w} v_{bow} \\cdot u_{w_c} + \\frac{\\partial}{\\partial u_w} \\log\\sum_{w_j\\in\\mathcal{V}} \\exp(v_{bow} \\cdot u_{w_j}) \\\\\n",
        "&= \\frac {1} {\\sum_{w_j\\in\\mathcal{V}} \\exp(v_{bow} \\cdot u_{w_j})}  \\frac{\\partial}{\\partial u_w} \\sum_{w_j\\in\\mathcal{V}} \\exp(v_{bow} \\cdot u_{w_j}) \\\\\n",
        "&= \\frac {1} {\\sum_{w_j\\in\\mathcal{V}} \\exp(v_{bow} \\cdot u_{w_j})}  \\frac{\\partial}{\\partial u_w} \\exp(v_{bow} \\cdot u_w) \\\\\n",
        "&= \\frac {\\exp(v_{bow} \\cdot u_w) } {\\sum_{w_j\\in\\mathcal{V}} \\exp(v_{bow} \\cdot u_{w_j})}  \\frac{\\partial}{\\partial u_w} v_{bow} \\cdot u_w \\\\\n",
        "&= p(w|v_{bow}) v_{bow} \\\\\n",
        "\\end{align}\n",
        "\n",
        "Recall $v_{bow} = v_{w_{-2}} + v_{w_{-1}}+ v_{w_{1}}+ v_{w_{2}}$. For each $v \\in \\{v_{w_{-2}}, v_{w_{-1}}, v_{w_{1}}, v_{w_{2}}\\}$\n",
        "\\begin{align}\n",
        "\\frac{\\partial}{\\partial v} - \\log p(w_c|v_{bow})\n",
        "&= \\frac{\\partial}{\\partial v} - \\log \\frac {\\exp(v_{bow} \\cdot u_{w_c})} {\\sum_{w_j\\in\\mathcal{V}} \\exp(v_{bow} \\cdot u_{w_j})} \\\\\n",
        "&= \\frac{\\partial}{\\partial v} - v_{bow} \\cdot u_{w_c} + \\log\\sum_{w_j\\in\\mathcal{V}} \\exp(v_{bow} \\cdot u_{w_j}) \\\\\n",
        "&= - \\frac{\\partial}{\\partial v} v_{bow} \\cdot u_{w_c} + \\frac{\\partial}{\\partial v} \\log\\sum_{w_j\\in\\mathcal{V}} \\exp(v_{bow} \\cdot u_{w_j}) \\\\\n",
        "&= - u_{w_c} + \\frac {1} {\\sum_{w_j\\in\\mathcal{V}} \\exp(v_{bow} \\cdot u_{w_j})}  \\frac{\\partial}{\\partial v} \\sum_{w_j\\in\\mathcal{V}} \\exp(v_{bow} \\cdot u_{w_j}) \\\\\n",
        "&= - u_{w_c} + \\sum_{w_j\\in\\mathcal{V}} \\frac {1} {\\sum_{w_j\\in\\mathcal{V}} \\exp(v_{bow} \\cdot u_{w_j})} \\frac{\\partial}{\\partial v} \\exp(v_{bow} \\cdot u_{w_j}) \\\\\n",
        "&= - u_{w_c} + \\sum_{w_j\\in\\mathcal{V}} \\frac {\\exp(v_{bow} \\cdot u_{w_j})} {\\sum_{w_j\\in\\mathcal{V}} \\exp(v_{bow} \\cdot u_{w_j})} \\frac{\\partial}{\\partial v} v_{bow} \\cdot u_{w_j}  \\\\\n",
        "&= - u_{w_c} + \\sum_{w_j\\in\\mathcal{V}} p(w_j|v_{bow}) u_{w_j} \\\\\n",
        "\\end{align}\n"
      ]
    }
  ]
}