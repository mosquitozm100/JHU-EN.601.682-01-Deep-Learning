{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW2 Release.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkANmyasP2bY",
        "colab_type": "text"
      },
      "source": [
        "# Q1: Scalar Computation Graph and Gradient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJ6_aDcWQYg3",
        "colab_type": "text"
      },
      "source": [
        "## TODO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rgc6AVBQXQq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def g_w1(x1: float, x2: float, w1: float, w2: float) -> float:\n",
        "    g = 0\n",
        "    raise NotImplementedError\n",
        "    return g\n",
        "\n",
        "def g_w2(x1: float, x2: float, w1: float, w2: float) -> float:\n",
        "    g = 0\n",
        "    raise NotImplementedError\n",
        "    return g\n",
        "\n",
        "def g_x1(x1: float, x2: float, w1: float, w2: float) -> float:\n",
        "    g = 0\n",
        "    raise NotImplementedError\n",
        "    return g\n",
        "\n",
        "def g_x2(x1: float, x2: float, w1: float, w2: float) -> float:\n",
        "    g = 0\n",
        "    raise NotImplementedError\n",
        "    return g"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFLw7UolQIe0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x1 = 0.2\n",
        "x2 = 0.4\n",
        "w1 = 0.3\n",
        "w2 = -0.5\n",
        "\n",
        "grads_x1 = []\n",
        "grads_x2 = []\n",
        "grads_w1 = []\n",
        "grads_w2 = []\n",
        "w1s = [w1]\n",
        "w2s = [w2]\n",
        "\n",
        "for i in range(30):\n",
        "    grad_w1 = g_w1(x1, x2, w1, w2)    \n",
        "    grad_w2 = g_w2(x1, x2, w1, w2)\n",
        "    \n",
        "    w2 += -0.01 * grad_w2\n",
        "    w1 += -0.01 * grad_w1\n",
        "    \n",
        "    w1s.append(w1)\n",
        "    w2s.append(w2)\n",
        "\n",
        "    grads_w1.append(grad_w1)\n",
        "    grads_w2.append(grad_w2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAsH7KUcQp8z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8V5EPucOdwu",
        "colab_type": "text"
      },
      "source": [
        "# Q3: Neural Network From Scratch\n",
        "\n",
        "In this section, we will implement a neural network to solve MNIST, a hand-written digits (0-9) classification dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eS5VOSqPUgu",
        "colab_type": "text"
      },
      "source": [
        "## Set up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNQLW-GH1bX-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from typing import Dict\n",
        "from functools import partial\n",
        "\n",
        "from tqdm import tqdm\n",
        "tqdm.monitor_interval = 0\n",
        "tqdm = partial(tqdm, bar_format='{l_bar}{r_bar}')\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSRfAdmy1uhn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TRAIN_SIZE = 50_000\n",
        "\n",
        "train = torchvision.datasets.MNIST('./data', train=True, transform=None, target_transform=None, download=True)\n",
        "test = torchvision.datasets.MNIST('./data', train=False, transform=None, target_transform=None, download=True)\n",
        "\n",
        "train_x = train.data.float().numpy()\n",
        "train_y = train.targets.numpy()\n",
        "\n",
        "shuffle_idx = np.arange(len(train_x))\n",
        "np.random.RandomState(0).shuffle(shuffle_idx)\n",
        "train_x = train_x[shuffle_idx]\n",
        "train_y = train_y[shuffle_idx]\n",
        "\n",
        "dev_x, dev_y = train_x[TRAIN_SIZE:], train_y[TRAIN_SIZE:]\n",
        "train_x, train_y = train_x[:TRAIN_SIZE], train_y[:TRAIN_SIZE]\n",
        "\n",
        "test_x = test.data.float().numpy()\n",
        "test_y = test.targets.numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GSBMEzy7sIV",
        "colab_type": "text"
      },
      "source": [
        "Sample of images\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRPzeyY-7a5J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "i = 4048 #@param {type: \"slider\", min: 0, max: 10000}\n",
        "print(f'label = {train_y[i]}')\n",
        "plt.imshow(train_x[i])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDrCxDQD8XfX",
        "colab_type": "text"
      },
      "source": [
        "Each images have the same shape of 28-by-28. We flatten the matrix and pretend it is just one long vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owHStQK_7UTW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NB_FEAT = 28 * 28"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qq9_DDRf4Gli",
        "colab_type": "text"
      },
      "source": [
        "Normalize the feature. Note that we only use the training set to compute the mean and standard derivation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkumLLwa2DP9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean = train_x.mean()\n",
        "std = train_x.std()\n",
        "\n",
        "train_x = (train_x - mean) / (std + 1e-7)\n",
        "dev_x = (dev_x - mean) / (std + 1e-7)\n",
        "test_x = (test_x - mean) / (std + 1e-7)\n",
        "\n",
        "train_x = train_x.reshape(-1, NB_FEAT)\n",
        "dev_x = dev_x.reshape(-1, NB_FEAT)\n",
        "test_x = test_x.reshape(-1, NB_FEAT)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNa2DCck51Jj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def row_logsumexp(x):\n",
        "    # numerical stablization\n",
        "    x_max = x.max(axis=1).reshape(-1, 1)\n",
        "    return x_max + np.log(np.exp(x - x_max).sum(axis=1)).reshape(-1, 1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vdk_ER0rPKBe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Parameters:\n",
        "    def __init__(self):\n",
        "        self.param: Dict[str, np.ndarray] = {}\n",
        "        self.grad: Dict[str, np.ndarray] = {}\n",
        "\n",
        "    def set_param(self, key: str, param: np.ndarray):\n",
        "        self.param[key] = param\n",
        "        self.grad[key] = np.zeros_like(param)\n",
        "\n",
        "    def get_param(self, key):\n",
        "        assert key in self.param, f'variable {key} is not part of the Parameter'\n",
        "        return self.param[key]\n",
        "\n",
        "    def accumlate_grad(self, key: str, grad: np.ndarray):\n",
        "        assert key in self.param, f'variable {key} is not part of the Parameter'\n",
        "        assert self.param[key].shape == grad.shape, f'for variable {key}, the shape of parameter and the shape of gradient is not matched'\n",
        "        self.grad[key] += grad\n",
        "\n",
        "    def zero_grad(self):\n",
        "        for key in self.param:\n",
        "            self.grad[key] = np.zeros_like(self.param[key])\n",
        "\n",
        "    def apply_grad(self, lr: float):\n",
        "        for key in self.param.keys():\n",
        "            assert self.param[key].shape == self.grad[key].shape, f'for variable {key}, the shape of parameter and the shape of gradient is not matched'\n",
        "            self.param[key] -= self.grad[key] * lr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raCPh5O_PKKY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def init_linear(input_dim, output_dim):\n",
        "    return np.random.RandomState(0).randn(input_dim, output_dim) * np.sqrt(2 / input_dim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkH7YxnoZ96D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main_training(params: Parameters, forward_and_backward, train_x, train_y, dev_x, dev_y, batch_size, learning_rate, nb_epochs):\n",
        "    assert isinstance(params, Parameters)\n",
        "    step = 0\n",
        "    best_acc = 0\n",
        "    for epoch in tqdm(range(nb_epochs)):\n",
        "        train_loss = []\n",
        "        lr = learning_rate\n",
        "        for i in range(0, len(train_x), batch_size):\n",
        "            batch_x = train_x[i:i+batch_size]\n",
        "            batch_y = train_y[i:i+batch_size]\n",
        "\n",
        "            loss, probs = forward_and_backward(batch_x, batch_y, params)\n",
        "            params.apply_grad(lr)\n",
        "\n",
        "            train_loss.append(loss)\n",
        "            step += 1\n",
        "\n",
        "        accs = []\n",
        "        for i in range(0, len(dev_x), batch_size):\n",
        "            batch_x = dev_x[i:i+batch_size]\n",
        "            batch_y = dev_y[i:i+batch_size]\n",
        "\n",
        "            loss, probs = forward_and_backward(batch_x, batch_y, params, grad=False)\n",
        "            pred_y = probs.argmax(axis=1)\n",
        "            accs.append(pred_y == batch_y)\n",
        "        acc = np.mean(accs) * 100\n",
        "        print(f'epoch {epoch} train loss = {np.mean(train_loss):.3f} dev accuracy = {acc:.2f}%')\n",
        "        if acc > best_acc:\n",
        "            best_acc = acc\n",
        "\n",
        "    print(f'best dev accuracy = {best_acc:.2f}%')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KslzAMpabAiV",
        "colab_type": "text"
      },
      "source": [
        "## Q2.1 Linear Classifier\n",
        "\n",
        "Let $x \\in R^{1\\times N}$ be a single sample. $N$ is the number of the feature dimension and $C$ is the number of class in the final classification. Note $N=28*28$ and $C=10$\n",
        "\n",
        "In the softmax layer, $W_{sm} \\in R^{N \\times C}$. The unnormalized score $s \\in R^{C}$ is\n",
        "\n",
        "$$s = x\\cdot W_{sm}$$\n",
        "\n",
        "With softmax function, we get the probability $p(c_i)$ of class $i$ where $i \\in \\{1,\\cdots,C\\}$\n",
        "\n",
        "$$p(c_i) = \\frac {\\exp(s_i)} {\\sum_i \\exp(s_i)}$$\n",
        "\n",
        "where $s_i$ is the $i^{th}$ element of $s$.\n",
        "\n",
        "Assuming $y \\in \\{1,\\cdots,C\\}$, we want to minimize $-\\log p(c_y)$, the cross-entropy loss between one hot true distribution `q(c_i) = 1 if i == y else 0` and $p(c_i)$.\n",
        "\n",
        "We will take the average $-\\log p(c_y)$ of a batch of x as the loss.\n",
        "\n",
        "In `linear_classifier_forward_and_backward`, `batch_x` and `batch_y` are matrixs, where each row are $x$ and $y$, respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAD_MC9lbELf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "linear_classifier = Parameters()\n",
        "linear_classifier.set_param('w_sm', init_linear(NB_FEAT, 10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpjfQEn_xjI6",
        "colab_type": "text"
      },
      "source": [
        "### TODO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aICn5CtgbMMt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def linear_classifier_forward_and_backward(batch_x, batch_y, param: Parameters, grad=True) -> (float, np.ndarray):\n",
        "    '''\n",
        "    compute the loss and the gradient of each parameter\n",
        "    return loss (float) and probs p(c_i) (matrix with shape batch_size x C)\n",
        "    '''\n",
        "    param.zero_grad()\n",
        "    w_sm = param.get_param('w_sm')\n",
        "\n",
        "    # compute loss and probs\n",
        "    loss = 0\n",
        "    probs = np.zeros((len(batch_x), 10))\n",
        "    raise NotImplementedError\n",
        "\n",
        "    if not grad:\n",
        "        return loss, probs\n",
        "\n",
        "    # compute gradient\n",
        "    w_sm_grad = np.zeros_like(w_sm)\n",
        "    raise NotImplementedError\n",
        "\n",
        "    # save the gradient\n",
        "    param.accumlate_grad('w_sm', w_sm_grad)\n",
        "    return loss, probs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pbBBpkpbbMz",
        "colab_type": "text"
      },
      "source": [
        "### Loss & Gradient Check\n",
        "\n",
        "If all assertions are passed, it means you are good to go."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3tTXJTrbfek",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use a prime number for batch size would make debugging easier\n",
        "bs = 7\n",
        "batch_x = train_x[:bs]\n",
        "batch_y = train_y[:bs]\n",
        "loss, _ = linear_classifier_forward_and_backward(batch_x, batch_y, linear_classifier)\n",
        "\n",
        "w_sm = linear_classifier.get_param('w_sm')\n",
        "_w_sm = torch.tensor(w_sm, dtype=torch.double, requires_grad=True)\n",
        "_batch_x = torch.tensor(batch_x, dtype=torch.double, requires_grad=False)\n",
        "_batch_y = torch.tensor(batch_y, dtype=torch.long, requires_grad=False)\n",
        "\n",
        "_logits = F.linear(_batch_x, _w_sm.T)\n",
        "_logprobs = F.log_softmax(_logits, dim=-1)\n",
        "_loss = -_logprobs[torch.arange(len(_batch_y)), _batch_y].mean()\n",
        "_loss.backward()\n",
        "\n",
        "assert np.isclose(loss, _loss.item())\n",
        "assert np.allclose(_w_sm.grad.numpy(), linear_classifier.grad['w_sm'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egX8ucvXbsP7",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7C_vFBwbvGG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BS = 50\n",
        "LR = 0.005\n",
        "NB_EPOCH = 20\n",
        "\n",
        "linear_classifier = Parameters()\n",
        "linear_classifier.set_param('w_sm', init_linear(NB_FEAT, 10))\n",
        "\n",
        "main_training(linear_classifier, linear_classifier_forward_and_backward, train_x, train_y, dev_x, dev_y, BS, LR, NB_EPOCH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkaYMGxZOqT1",
        "colab_type": "text"
      },
      "source": [
        "## Q2.1 MLP with Single Hidden Layer\n",
        "\n",
        "Let $x \\in R^{1\\times N}$ be a single sample. $N$ is the number of the feature dimension and $C$ is the number of class in the final classification.\n",
        "\n",
        "For the first layer, $W_1 \\in R^{N\\times \\frac{N}{2}}$ and $b_1 \\in R^{\\frac{N}{2}}$\n",
        "\n",
        "$$\\bar{h}_1 = x\\cdot W_1 + b_1$$\n",
        "\n",
        "$$h_1 = ReLU(\\bar{h}_1)$$\n",
        "\n",
        "In the softmax layer, $W_{sm} \\in R^{\\frac{N}{2} \\times C}$. The unnormalized score $s \\in R^{C}$ is\n",
        "\n",
        "$$s = h_1\\cdot W_{sm}$$\n",
        "\n",
        "With softmax function, we get the probability $p(c_i)$ of class $i$ where $i \\in \\{1,\\cdots,C\\}$\n",
        "\n",
        "$$p(c_i) = \\frac {\\exp(s_i)} {\\sum_i \\exp(s_i)}$$\n",
        "\n",
        "where $s_i$ is the $i^{th}$ element of $s$.\n",
        "\n",
        "Assuming $y \\in \\{1,\\cdots,C\\}$, we want to minimize $-\\log p(c_y)$, the cross-entropy loss between one hot true distribution `q(c_i) = 1 if i == y else 0` and $p(c_i)$.\n",
        "\n",
        "We will take the average $-\\log p(c_y)$ of a batch of x as the loss.\n",
        "\n",
        "In `mlp_single_hidden_forward_and_backward`, `batch_x` and `batch_y` are matrixs, where each row are $x$ and $y$, respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQlWUL_f520s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mlp_single_hidden = Parameters()\n",
        "mlp_single_hidden.set_param('w1', init_linear(NB_FEAT, NB_FEAT // 2))\n",
        "mlp_single_hidden.set_param('b1', np.zeros(NB_FEAT // 2))\n",
        "mlp_single_hidden.set_param('w_sm', init_linear(NB_FEAT // 2, 10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37K-xIMFxeZo",
        "colab_type": "text"
      },
      "source": [
        "### TODO\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12xq8CDgCZYX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mlp_single_hidden_forward_and_backward(batch_x, batch_y, param: Parameters, grad=True):\n",
        "    '''\n",
        "    compute the loss and the gradient of each parameter\n",
        "    return loss (float) and p(c_i) (matrix with shape batch_size x C)\n",
        "    '''\n",
        "    param.zero_grad()\n",
        "    w1 = param.get_param('w1')\n",
        "    b1 = param.get_param('b1')\n",
        "    w_sm = param.get_param('w_sm')\n",
        "\n",
        "\n",
        "    # compute loss and probs\n",
        "    loss = 0\n",
        "    probs = np.zeros((len(batch_x), 10))\n",
        "    raise NotImplementedError\n",
        "\n",
        "    if not grad:\n",
        "        return loss, probs\n",
        "\n",
        "    # compute gradient\n",
        "    w1_grad = np.zeros_like(w1)\n",
        "    b1_grad = np.zeros_like(b1)\n",
        "    w_sm_grad = np.zeros_like(w_sm)\n",
        "    raise NotImplementedError\n",
        "\n",
        "    param.accumlate_grad('w1', w1_grad)\n",
        "    param.accumlate_grad('b1', b1_grad)\n",
        "    param.accumlate_grad('w_sm', w_sm_grad)\n",
        "    return loss, probs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vpqro3iO0Am",
        "colab_type": "text"
      },
      "source": [
        "### Loss & Gradient Check\n",
        "\n",
        "If all assertions are passed, it means you are good to go."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAl0A2_nCx-y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use a prime number for batch size would make debugging easier\n",
        "bs = 7\n",
        "batch_x = train_x[:bs]\n",
        "batch_y = train_y[:bs]\n",
        "loss, _ = mlp_single_hidden_forward_and_backward(batch_x, batch_y, mlp_single_hidden)\n",
        "\n",
        "w1 = mlp_single_hidden.get_param('w1')\n",
        "b1 = mlp_single_hidden.get_param('b1')\n",
        "w_sm = mlp_single_hidden.get_param('w_sm')\n",
        "_w1 = torch.tensor(w1, dtype=torch.double, requires_grad=True)\n",
        "_b1 = torch.tensor(b1, dtype=torch.double, requires_grad=True)\n",
        "_w_sm = torch.tensor(w_sm, dtype=torch.double, requires_grad=True)\n",
        "_batch_x = torch.tensor(batch_x, dtype=torch.double, requires_grad=False)\n",
        "_batch_y = torch.tensor(batch_y, dtype=torch.long, requires_grad=False)\n",
        "\n",
        "_h1 = F.linear(_batch_x, _w1.T) + _b1\n",
        "_h1 = F.relu(_h1)\n",
        "_logits = F.linear(_h1, _w_sm.T)\n",
        "_logprobs = F.log_softmax(_logits, dim=-1)\n",
        "_loss = -_logprobs[torch.arange(len(_batch_y)), _batch_y].mean()\n",
        "_loss.backward()\n",
        "\n",
        "assert np.isclose(loss, _loss.item())\n",
        "assert np.allclose(_w_sm.grad.numpy(), mlp_single_hidden.grad['w_sm'])\n",
        "assert np.allclose(_w1.grad.numpy(), mlp_single_hidden.grad['w1'])\n",
        "assert np.allclose(_b1.grad.numpy(), mlp_single_hidden.grad['b1'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PaEBK2p_azU6",
        "colab_type": "text"
      },
      "source": [
        "### Train!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INoZSBXT52_o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BS = 50\n",
        "LR = 0.005\n",
        "NB_EPOCH = 20\n",
        "\n",
        "mlp_single_hidden = Parameters()\n",
        "mlp_single_hidden.set_param('w1', init_linear(NB_FEAT, NB_FEAT // 2))\n",
        "mlp_single_hidden.set_param('b1', np.zeros(NB_FEAT // 2))\n",
        "mlp_single_hidden.set_param('w_sm', init_linear(NB_FEAT // 2, 10))\n",
        "\n",
        "main_training(mlp_single_hidden, mlp_single_hidden_forward_and_backward, train_x, train_y, dev_x, dev_y, BS, LR, NB_EPOCH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siWRroXQHSSo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14ZhYaVwHSRE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzyRX0-fcCBs",
        "colab_type": "text"
      },
      "source": [
        "## Q2.3 MLP with Two Hidden Layer\n",
        "\n",
        "Let $x \\in R^{1\\times N}$ be a single sample. $N$ is the number of the feature dimension and $C$ is the number of class in the final classification.\n",
        "\n",
        "For the first layer, $W_1 \\in R^{N\\times \\frac{N}{2}}$ and $b_1 \\in R^{\\frac{N}{2}}$\n",
        "\n",
        "$$\\bar{h}_1 = x\\cdot W_1 + b_1$$\n",
        "\n",
        "$$h_1 = ReLU(\\bar{h}_1)$$\n",
        "\n",
        "For the second layer, $W_2 \\in R^{\\frac{N}{2} \\times \\frac{N}{2}}$ and $b_2 \\in R^{\\frac{N}{2}}$\n",
        "\n",
        "$$\\bar{h}_2 = h_1\\cdot W_2 + b_2$$\n",
        "\n",
        "$$h_2 = h_1 + ReLU(\\bar{h}_2)$$\n",
        "\n",
        "*Sidenote: in deep learning, $g(x, f) = x + f(x)$ is usually referred to as skip connection.*\n",
        "\n",
        "In the softmax layer, $W_{sm} \\in R^{\\frac{N}{2} \\times C}$. The unnormalized score $s \\in R^{C}$ is\n",
        "\n",
        "$$s = h_2\\cdot W_{sm}$$\n",
        "\n",
        "With softmax function, we get the probability $p(c_i)$ of class $i$ where $i \\in \\{1,\\cdots,C\\}$\n",
        "\n",
        "$$p(c_i) = \\frac {\\exp(s_i)} {\\sum_i \\exp(s_i)}$$\n",
        "\n",
        "where $s_i$ is the $i^{th}$ element of $s$.\n",
        "\n",
        "Assuming $y \\in \\{1,\\cdots,C\\}$, we want to minimize $-\\log p(c_y)$, the cross-entropy loss between one hot true distribution `q(c_i) = 1 if i == y else 0` and $p(c_i)$.\n",
        "\n",
        "We will take the average $-\\log p(c_y)$ of a batch of x as the loss.\n",
        "\n",
        "In `mlp_two_hidden_forward_and_backward`, `batch_x` and `batch_y` are matrixs, where each row are $x$ and $y$, respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1XBElNccHeu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mlp_two_hidden = Parameters()\n",
        "mlp_two_hidden.set_param('w1', init_linear(NB_FEAT, NB_FEAT // 2))\n",
        "mlp_two_hidden.set_param('b1', np.zeros(NB_FEAT // 2))\n",
        "mlp_two_hidden.set_param('w2', init_linear(NB_FEAT // 2, NB_FEAT // 2))\n",
        "mlp_two_hidden.set_param('b2', np.zeros(NB_FEAT // 2))\n",
        "mlp_two_hidden.set_param('w_sm', init_linear(NB_FEAT // 2, 10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiVRXqN4xXgm",
        "colab_type": "text"
      },
      "source": [
        "### TODO\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tsJbpmgcHh6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mlp_two_hidden_forward_and_backward(batch_x, batch_y, param: Parameters, grad=True):\n",
        "    '''\n",
        "    compute the loss and the gradient of each parameter\n",
        "    return loss (float) and p(c_i) (matrix with shape batch_size x C)\n",
        "    '''\n",
        "    param.zero_grad()\n",
        "    w1 = param.get_param('w1')\n",
        "    b1 = param.get_param('b1')\n",
        "    w2 = param.get_param('w2')\n",
        "    b2 = param.get_param('b2')\n",
        "    w_sm = param.get_param('w_sm')\n",
        "\n",
        "    # compute loss and probs\n",
        "    loss = 0\n",
        "    probs = np.zeros((len(batch_x), 10))\n",
        "    raise NotImplementedError\n",
        "\n",
        "    if not grad:\n",
        "        return loss, probs\n",
        "\n",
        "    # compute gradient\n",
        "    w1_grad = np.zeros_like(w1)\n",
        "    b1_grad = np.zeros_like(b1)\n",
        "    w2_grad = np.zeros_like(w2)\n",
        "    b2_grad = np.zeros_like(b2)\n",
        "    w_sm_grad = np.zeros_like(w_sm)\n",
        "    raise NotImplementedError\n",
        "\n",
        "    param.accumlate_grad('w1', w1_grad)\n",
        "    param.accumlate_grad('b1', b1_grad)\n",
        "    param.accumlate_grad('w2', w2_grad)\n",
        "    param.accumlate_grad('b2', b2_grad)\n",
        "    param.accumlate_grad('w_sm', w_sm_grad)\n",
        "    return loss, probs\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BCF5GRKvdGaw"
      },
      "source": [
        "### Loss & Gradient Check\n",
        "\n",
        "If all assertions are passed, it means you are good to go."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnkCUnV6cHlC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use a prime number for batch size would make debugging easier\n",
        "bs = 7\n",
        "batch_x = train_x[:bs]\n",
        "batch_y = train_y[:bs]\n",
        "loss, _ = mlp_two_hidden_forward_and_backward(batch_x, batch_y, mlp_two_hidden)\n",
        "\n",
        "w1 = mlp_two_hidden.get_param('w1')\n",
        "b1 = mlp_two_hidden.get_param('b1')\n",
        "w2 = mlp_two_hidden.get_param('w2')\n",
        "b2 = mlp_two_hidden.get_param('b2')\n",
        "w_sm = mlp_two_hidden.get_param('w_sm')\n",
        "_w1 = torch.tensor(w1, dtype=torch.double, requires_grad=True)\n",
        "_b1 = torch.tensor(b1, dtype=torch.double, requires_grad=True)\n",
        "_w2 = torch.tensor(w2, dtype=torch.double, requires_grad=True)\n",
        "_b2 = torch.tensor(b2, dtype=torch.double, requires_grad=True)\n",
        "_w_sm = torch.tensor(w_sm, dtype=torch.double, requires_grad=True)\n",
        "_batch_x = torch.tensor(batch_x, dtype=torch.double, requires_grad=False)\n",
        "_batch_y = torch.tensor(batch_y, dtype=torch.long, requires_grad=False)\n",
        "\n",
        "_h1 = F.linear(_batch_x, _w1.T) + _b1\n",
        "_h1 = F.relu(_h1)\n",
        "\n",
        "_h2 = F.linear(_h1, _w2.T) + _b2\n",
        "_h2 = F.relu(_h2)\n",
        "_h2 = _h2 + _h1\n",
        "\n",
        "_logits = F.linear(_h2, _w_sm.T)\n",
        "_logprobs = F.log_softmax(_logits, dim=-1)\n",
        "_loss = -_logprobs[torch.arange(len(_batch_y)), _batch_y].mean()\n",
        "_loss.backward()\n",
        "\n",
        "assert np.isclose(loss, _loss.item())\n",
        "assert np.allclose(_w_sm.grad.numpy(), mlp_two_hidden.grad['w_sm'])\n",
        "assert np.allclose(_w2.grad.numpy(), mlp_two_hidden.grad['w2'])\n",
        "assert np.allclose(_b2.grad.numpy(), mlp_two_hidden.grad['b2'])\n",
        "assert np.allclose(_w1.grad.numpy(), mlp_two_hidden.grad['w1'])\n",
        "assert np.allclose(_b1.grad.numpy(), mlp_two_hidden.grad['b1'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fNjUcsPoenly"
      },
      "source": [
        "### Train!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bnIsb-heqWE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BS = 50\n",
        "LR = 0.005\n",
        "NB_EPOCH = 20\n",
        "\n",
        "mlp_two_hidden = Parameters()\n",
        "mlp_two_hidden.set_param('w1', init_linear(NB_FEAT, NB_FEAT // 2))\n",
        "mlp_two_hidden.set_param('b1', np.zeros(NB_FEAT // 2))\n",
        "mlp_two_hidden.set_param('w2', init_linear(NB_FEAT // 2, NB_FEAT // 2))\n",
        "mlp_two_hidden.set_param('b2', np.zeros(NB_FEAT // 2))\n",
        "mlp_two_hidden.set_param('w_sm', init_linear(NB_FEAT // 2, 10))\n",
        "\n",
        "main_training(mlp_two_hidden, mlp_two_hidden_forward_and_backward, train_x, train_y, dev_x, dev_y, BS, LR, NB_EPOCH)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
