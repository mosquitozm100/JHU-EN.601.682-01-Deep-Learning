{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "logistic_regression_mlp_pytorch (release).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Cq1t9bGsDTvi"
      },
      "source": [
        "Do not delete this cell. It defines custom LaTeX commands.\n",
        "$$\n",
        "\\newcommand{\\xb}{\\boldsymbol{x}}\n",
        "\\newcommand{\\wb}{\\boldsymbol{w}}\n",
        "\\newcommand{\\pb}{\\boldsymbol{p}}\n",
        "\\newcommand{\\1}{\\mathbb{1}}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bfyI_9zZPwsa"
      },
      "source": [
        "# Logistic Regression and MLPs with PyTorch\n",
        "\n",
        "Here you will continue to classify images as either 4s or 9s.\n",
        "\n",
        "You will:\n",
        "*   Implement the logistic-regression model in PyTorch\n",
        "*   Create a similar multilayer perceptron with 50 hidden units in PyTorch\n",
        "\n",
        "And you will see which leads to a lower error rate on a held-out validation set.\n",
        "\n",
        "**Note:** We will be using PyTorch's autograd feature behind the scenes in this notebook, you can follow this [tutorial](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html) to get a better understanding of it. Until now we have been implementing both the forward and backward functions for all of our machine learning models, PyTorch's autograd is like magic, that demands us to only implement the forward function while it automatically computes the gradients and hence the backward fucntion too for us."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mKqJ2q6XalPc",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "\n",
        "from pathlib import Path\n",
        "HOME = Path.home()\n",
        "MNIST_PATH = HOME / 'data' / 'mnist'\n",
        "\n",
        "NUM_CLASSES = 10\n",
        "CHANNELS = 1\n",
        "HEIGHT = 28\n",
        "WIDTH = 28"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ThF_mw3iasJ2",
        "colab": {}
      },
      "source": [
        "# We're going to load the official train set and never touch\n",
        "# the true test set in these experiments, which consists of 10,000\n",
        "# separate examples. We'll instead split our training set into\n",
        "# a set for training and a set for validation.\n",
        "official_mnist_train = torchvision.datasets.MNIST(str(MNIST_PATH), train=True, download=True)\n",
        "official_train_images = official_mnist_train.train_data.numpy().astype(np.float32)\n",
        "official_train_labels = official_mnist_train.train_labels.numpy().astype(np.int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cdhHhkpFxTUV",
        "colab": {}
      },
      "source": [
        "print(official_train_images.shape)\n",
        "print(official_train_labels.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IoK_fYnT2u-g"
      },
      "source": [
        "First let's grab only the 4s and 9s, which will end up forming our binary classification problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xBsaqsTlbwcf",
        "colab": {}
      },
      "source": [
        "mask_4_9 = (official_train_labels == 4) | (official_train_labels == 9)\n",
        "images_4_9 = official_train_images[mask_4_9]\n",
        "labels_4_9 = (official_train_labels[mask_4_9] == 9).astype(np.int)\n",
        "print(images_4_9.shape, labels_4_9.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2yVKLOn8_EMD"
      },
      "source": [
        "Let's view a few examples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RJxxlEjhcpW9",
        "colab": {}
      },
      "source": [
        "example_images = np.concatenate(images_4_9[:10], axis=1)\n",
        "example_labels = labels_4_9[:10]\n",
        "print(example_labels)\n",
        "plt.imshow(example_images)\n",
        "plt.grid(False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "toe9gJyK_LBO"
      },
      "source": [
        "Here we'll split our training set into 10000 for training and the rest for validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "E60IytczeT22",
        "colab": {}
      },
      "source": [
        "train_images, val_images = np.split(images_4_9, [10000])\n",
        "train_labels, val_labels = np.split(labels_4_9, [10000])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4uReKYU4y5k0",
        "colab": {}
      },
      "source": [
        "print(train_images.shape, train_labels.shape)\n",
        "print(val_images.shape, val_labels.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ocXrRfsX_VwM"
      },
      "source": [
        "And we'll normalize our data in one of the simplest ways possible: centering and scaling on an image-by-image basis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZJsU5u9WhBzN",
        "colab": {}
      },
      "source": [
        "def normalize_stats_image_by_image(images):\n",
        "  mean = images.mean(axis=(1,2), keepdims=True)\n",
        "  stdev = images.std(axis=(1,2), keepdims=True)\n",
        "  return (images - mean) / stdev"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dL9XyAMYk8qs",
        "colab": {}
      },
      "source": [
        "train_images = normalize_stats_image_by_image(train_images)\n",
        "val_images = normalize_stats_image_by_image(val_images)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lgIXQ0QO0NUy",
        "colab": {}
      },
      "source": [
        "print(train_images[:3].mean(axis=(1, 2)))\n",
        "print(train_images[:3].std(axis=(1, 2)))\n",
        "print(val_images[:3].mean(axis=(1, 2)))\n",
        "print(val_images[:3].std(axis=(1, 2)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zXeAIHWozzso"
      },
      "source": [
        "Let's view some of the training and validation examples, all concatenated together so that they share the same color range (without any additional tweaking)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9VQgqbB7bUFz",
        "colab": {}
      },
      "source": [
        "some_train_images = np.concatenate(train_images[:10], axis=1)\n",
        "some_val_images = np.concatenate(val_images[:10], axis=1)\n",
        "some_train_and_val_images = np.concatenate([some_train_images, some_val_images], axis=0)\n",
        "plt.imshow(some_train_and_val_images)\n",
        "plt.grid(False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wZlPIpgSX26w"
      },
      "source": [
        "Let's also reshape all examples, under the assumption that *our models will always accept vectors rather than images.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BmXdidmRm0jh",
        "colab": {}
      },
      "source": [
        "train_vectors = train_images.reshape(-1, HEIGHT * WIDTH)\n",
        "val_vectors = val_images.reshape(-1, HEIGHT * WIDTH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vMgpDZIlWnko"
      },
      "source": [
        "**Answer each of the following questions in the corresponding Markdown cells:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qQm-kC4OWnNn"
      },
      "source": [
        "**Look up the documentation for `torch.from_numpy`. What does it return?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jTfXeBDYWm5e"
      },
      "source": [
        "Answer: **TODO**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BH7rYv5oWmg5"
      },
      "source": [
        "**What does the `requires_grad` parameter of `Tensor` do ?** Hint: Take a look at https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PmWbExBjHPc1"
      },
      "source": [
        "**Complete the following function, which retrieves a batch of examples.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VV-22bUNDRpO",
        "colab": {}
      },
      "source": [
        "def batch(batch_size, training=True):\n",
        "  \"\"\"Create a batch of examples.\n",
        "  \n",
        "  This creates a batch of inputs and a batch of corresponding\n",
        "  ground-truth labels.\n",
        "  \n",
        "  Args:\n",
        "    batch_size: An integer.\n",
        "    training: A boolean. If True, grab examples from the training\n",
        "      set; otherwise, grab them from the validation set.\n",
        "  \n",
        "  Returns:\n",
        "    A tuple,\n",
        "    input_batch: A Tensor of floats with shape\n",
        "      [batch_size, num_features]\n",
        "    label_batch: A Tensor of ints with shape\n",
        "      [batch_size].\n",
        "  \"\"\"\n",
        "  if training:\n",
        "    random_ind = np.random.choice(train_vectors.shape[0], size=batch_size, replace=False)\n",
        "    input_batch = train_vectors[random_ind]\n",
        "    label_batch = train_labels[random_ind]\n",
        "  else:\n",
        "    input_batch = val_vectors[:batch_size]\n",
        "    label_batch = val_labels[:batch_size]\n",
        "  \n",
        "  # Create PyTorch Tensors from our NumPy arrays\n",
        "  # TODO: Fill in each line.\n",
        "  raise NotImplementedError\n",
        "  input_batch =  None\n",
        "  label_batch =  None\n",
        "  \n",
        "  return input_batch, label_batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "i3Fk_aJ7I497"
      },
      "source": [
        "**Answer each of the following questions in the corresponding Markdown cells:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QH2OjtzzJaDz"
      },
      "source": [
        "**What are the arguments for `torch.nn.Linear`? Describe each argument using a single sentence.** Hint: You can hit `tab` or you can search for `Linear` here: http://pytorch.org/docs/master/nn.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0pxTxmpIJa8s"
      },
      "source": [
        "Answer: **TODO**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iH7QKFjpJaQn"
      },
      "source": [
        "**Look at the example(s) for `Linear` in http://pytorch.org/docs/master/nn.html and answer: Why might it make sense to instantiate a `Linear` object that can later be called as a function?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rRZwoGXwJaat"
      },
      "source": [
        "Answer: **TODO**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wNZVAGm6HZaL"
      },
      "source": [
        "**Complete the following model, which maps examples from a dimensionality of `HEIGHT * WIDTH` to a dimensionality of 1 through a linear (actually, affine) transformation.**\n",
        "\n",
        "For any particular example $\\xb$, we'll end up modeling $p(y \\mid \\xb)$ as\n",
        "\n",
        "$$\n",
        "\\hat{p}(y = 0) = \\sigma(\\wb^T\\xb + b), \\qquad\n",
        "\\hat{p}(y = 1) = 1 - \\hat{p}(y = 0)\n",
        "$$\n",
        "\n",
        "where here the label $y = 0$ specifies that our image is a 4, and the label $y = 1$ specifies that our image is a 9.\n",
        "\n",
        "This model should compute $\\wb^T\\xb + b$.\n",
        "\n",
        "Hint: If you feel lost, take a look at the first example in http://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html ; the idea should help guide you to defining and using `Module`s from `torch.nn`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MVZjEc5FGzDn",
        "colab": {}
      },
      "source": [
        "class SimpleLinear(torch.nn.Module):\n",
        "  \"\"\"A simple linear model.\n",
        "  \n",
        "  Map from inputs of size [batch_size, num_features] to\n",
        "  outputs of size [batch_size, 1].\n",
        "  \"\"\"\n",
        "  \n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    raise NotImplementedError\n",
        "  \n",
        "  def forward(self, x):\n",
        "    raise NotImplementedError\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dCFHzTrhLoCA"
      },
      "source": [
        "Now, let's instantiate our model, which *is an object that can also be called as a function.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YhK3Dyy-LoLW",
        "colab": {}
      },
      "source": [
        "model = SimpleLinear()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "M2TOccbSM9wj"
      },
      "source": [
        "**In the following code cell, copy over**\n",
        "\n",
        "```\n",
        "for p in model.parameters():\n",
        "  print(p.size())\n",
        "```\n",
        "\n",
        "**and run it.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DmX2_0HMM4G7",
        "colab": {}
      },
      "source": [
        "raise NotImplementedError"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IMRkuEW9NTht"
      },
      "source": [
        "**Answer in the following Markdown cell: You should see two sizes here. (`size` in PyTorch is analagous to `shape` in NumPy.) Based on the sizes you see, what do these two `Parameters` correspond to?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "M7CkErgBN0AR"
      },
      "source": [
        "Answer: **TODO**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zEcJLwF4x8Bz"
      },
      "source": [
        "Let's look at a few examples (and predictions) before we train:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9XqFuqNLx8P3",
        "colab": {}
      },
      "source": [
        "example_inputs, example_labels = batch(batch_size=10, training=True)\n",
        "outputs = model(example_inputs)\n",
        "label_1_probs = torch.sigmoid(outputs)\n",
        "\n",
        "# Right now these are PyTorch Tensors. We can\n",
        "# convert these Tensors to NumPy arrays using `data.numpy()`.\n",
        "example_inputs, example_labels = example_inputs.numpy(), example_labels.numpy()\n",
        "label_1_probs = label_1_probs.numpy()\n",
        "\n",
        "# Also, let's transform the inputs back into images for visualization.\n",
        "example_images = example_inputs.reshape(-1, HEIGHT, WIDTH)\n",
        "plt.imshow(np.concatenate(example_images, axis=1))\n",
        "plt.grid(False)\n",
        "\n",
        "print(label_1_probs.flatten().round(2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6HLuNQ8N0xX1"
      },
      "source": [
        "**In the following Markdown cell, answer: Letting labels of 0 correspond to 4s and labels of 1 correspond to 9s, what fraction of the above probabilities would yield correct predictions?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gP7SyHEB0xgu"
      },
      "source": [
        "Answer: **TODO**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_DUdecM0N1fo"
      },
      "source": [
        "Let's create an optimizer, which keeps track of a particular set of parameters to optimizer over (here all of our model's parameters) and also takes steps according to some parameters (here just the learning rate)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "El2xSj-yLoVU",
        "colab": {}
      },
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hKDO5rBo1in5"
      },
      "source": [
        "Now let's define functions to a) take one training step and b) perform one validation \"step\" (without optimization).\n",
        "\n",
        "The loss for a *single* example is\n",
        "$$\n",
        "\\begin{align}\n",
        "l(\\hat{\\pb}, \\pb)\n",
        "&= -\\sum_i p_i \\log \\hat{p}_i\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "where\n",
        "\n",
        "$$\n",
        "\\hat{p}_1 = \\sigma(\\wb^T\\xb) \\qquad \\text{and} \\qquad \\hat{p}_0 = 1 - \\hat{p}_1\n",
        "$$\n",
        "\n",
        "Note that `binary_cross_entropy_with_logits` expects *logits* as inputs (which have never been passed to the sigmoid) but *probabilities* as targets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KxMEN0hsxMbm",
        "colab": {}
      },
      "source": [
        "def train_step(batch_size=100):\n",
        "  \n",
        "  # This enables a `training` flag which differentiates operation\n",
        "  # during training vs. operation during inference. Here we don't\n",
        "  # need it, but it's good practice to set it.\n",
        "  model.train()\n",
        "  \n",
        "  # Get a random training batch and compute corresponding outputs\n",
        "  # from our model.\n",
        "  input_batch, label_batch = batch(batch_size, training=True)\n",
        "  output_batch = model(input_batch)\n",
        "  \n",
        "  # Compute loss (and error rate).\n",
        "  # Note that our labels can be used as ground-truth probabilities,\n",
        "  # but also note that binary_cross_entropy_with_logits expects\n",
        "  # our outputs and ground-truth probabilities to have the same shape.\n",
        "  p_batch = label_batch.float().view(-1, 1)\n",
        "  loss = F.binary_cross_entropy_with_logits(output_batch, p_batch)  \n",
        "  pred_batch = (output_batch > 0).view(-1).long()\n",
        "  error_rate = 1.0 - (pred_batch == label_batch).float().mean()\n",
        "  \n",
        "  # Here's the magic of backpropagation: calling backward fills\n",
        "  # in the .grad attribute for all model parameters.\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  \n",
        "  # Take a step.\n",
        "  optimizer.step()\n",
        "  \n",
        "  return loss.item(), error_rate.item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AIrkc10LHD4Y"
      },
      "source": [
        "**In the following Markdown cell, answer: What would happen if we didn't reshape `p_batch` (using `view`) above?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uVWOnHp7HPmA"
      },
      "source": [
        "Answer: **TODO**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "h7JygaaR9KkO",
        "colab": {}
      },
      "source": [
        "def val():\n",
        "  \n",
        "  model.eval()\n",
        "  input_batch, label_batch = batch(len(val_vectors), training=False)\n",
        "  output_batch = model(input_batch)\n",
        "\n",
        "  p_batch = label_batch.float().view(-1, 1)\n",
        "  loss = F.binary_cross_entropy_with_logits(output_batch, p_batch)  \n",
        "  pred_batch = (output_batch > 0).view(-1).long()\n",
        "  error_rate = 1.0 - (pred_batch == label_batch).float().mean()\n",
        "  \n",
        "  return loss.item(), error_rate.item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "o_LCeeO92kLB"
      },
      "source": [
        "Finally, let's train, and also plot loss and error rate as a function of iteration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CKSCshLB87yc",
        "colab": {}
      },
      "source": [
        "# Let's make sure we always start from scratch (that is,\n",
        "# without starting from parameters from a previous run).\n",
        "for module in model.children():\n",
        "  module.reset_parameters()\n",
        "\n",
        "info = []\n",
        "fig, ax = plt.subplots(2, 1, sharex=True)\n",
        "num_steps = 5000\n",
        "num_steps_per_val = 50\n",
        "best_val_err = 1.0\n",
        "for step in range(num_steps):\n",
        "  train_loss, train_err = train_step()\n",
        "  if step % num_steps_per_val == 0:\n",
        "    val_loss, val_err = val()\n",
        "    if val_err < best_val_err:\n",
        "      best_val_err = val_err\n",
        "      print('Step {:5d}: Obtained a best validation error of {:.3f}.'.format(step, best_val_err))\n",
        "    info.append([step, train_loss, val_loss, train_err, val_err])\n",
        "    x, y11, y12, y21, y22 = zip(*info)\n",
        "    ax[0].plot(x, y11, x, y12)\n",
        "    ax[0].legend(['Train loss', 'Val loss'])\n",
        "    ax[1].plot(x, y21, x, y22)\n",
        "    ax[1].legend(['Train err', 'Val err'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kGhT34mj7igI",
        "colab": {}
      },
      "source": [
        "example_inputs, example_labels = batch(batch_size=10, training=True)\n",
        "outputs = model(example_inputs)\n",
        "label_1_probs = torch.sigmoid(outputs)\n",
        "\n",
        "# Right now these are PyTorch Tensors. We can\n",
        "# convert these Tensors to NumPy arrays using `data.numpy()`.\n",
        "example_inputs, example_labels = example_inputs.numpy(), example_labels.numpy()\n",
        "label_1_probs = label_1_probs.numpy()\n",
        "\n",
        "# Also, let's transform the inputs back into images for visualization.\n",
        "example_images = example_inputs.reshape(-1, HEIGHT, WIDTH)\n",
        "plt.imshow(np.concatenate(example_images, axis=1))\n",
        "plt.grid(False)\n",
        "\n",
        "print(label_1_probs.flatten().round(2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VnqxizLrBMRl"
      },
      "source": [
        "**In the following Markdown cell, answer: Letting labels of 0 correspond to 4s and labels of 1 correspond to 9s, what fraction of the above probabilities would yield correct predictions?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KhVuD7G0AE8U"
      },
      "source": [
        "Answer: **TODO**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xEKN6REc83da"
      },
      "source": [
        "**In the following Markdown Cell, answer:**\n",
        "\n",
        "**We took 5000 optimization steps, each with a batch size of 100 and a learning rate of 1e-4. Did the validation loss stabilize? What was the best validation error rate that you achieved?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BVLJuXfc839Y"
      },
      "source": [
        "Answer: **TODO**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULMkg5F3xGXR",
        "colab_type": "text"
      },
      "source": [
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9DUS7HHKGqVU"
      },
      "source": [
        "**Rerun the above experiment with various learning rates to minimize the error rate on the validation set. What was the best error rate you could achieve?**\n",
        "\n",
        "Hint: You should be able to hit an error rate of 3.5% or better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5ZTe21Gw0tpV"
      },
      "source": [
        "Answer: **TODO**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S-eSFHnH0n11",
        "colab": {}
      },
      "source": [
        "learning_rate = None\n",
        "raise NotImplementedError\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "for module in model.children():\n",
        "  module.reset_parameters()\n",
        "\n",
        "info = []\n",
        "fig, ax = plt.subplots(2, 1, sharex=True)\n",
        "num_steps = 5000\n",
        "num_steps_per_val = 50\n",
        "best_val_err = 1.0\n",
        "for step in range(num_steps):\n",
        "  train_loss, train_err = train_step()\n",
        "  if step % num_steps_per_val == 0:\n",
        "    val_loss, val_err = val()\n",
        "    if val_err < best_val_err:\n",
        "      best_val_err = val_err\n",
        "      print('Step {:5d}: Obtained a best validation error of {:.3f}.'.format(step, best_val_err))\n",
        "    info.append([step, train_loss, val_loss, train_err, val_err])\n",
        "    x, y11, y12, y21, y22 = zip(*info)\n",
        "    ax[0].plot(x, y11, x, y12)\n",
        "    ax[0].legend(['Train loss', 'Val loss'])\n",
        "    ax[1].plot(x, y21, x, y22)\n",
        "    ax[1].legend(['Train err', 'Val err'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wRqW1bkG6u3d"
      },
      "source": [
        "**Write code to visualize the weight matrix of your best model as a 28 x 28 image.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LjOTOVi8x6uL",
        "colab": {}
      },
      "source": [
        "raise NotImplementedError"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9sHRxJk07ns_"
      },
      "source": [
        "**In the following Markdown Cell, answer:**\n",
        "\n",
        "**Can you explain what you're seeing in this visualization of our weights? For example, what do you think the weights with large values (black in the image) correspond to?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "54q8nRCB2hnf"
      },
      "source": [
        "Answer: **TODO**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aqYR6R8YDboX"
      },
      "source": [
        "**Repeat the above experiments using a multilayer perceptron with 50 hidden units, using ReLU nonlinearities. Vary your learning rate as you wish to minimize the error rate on the validation set. What was the best error rate you were able to achieve?**\n",
        "\n",
        "Note that **you do not need to reanswer questions and you do not need to repeat visualizations.** You just need to implement the MLP model, run training (potentially for various learning rates), and report your results. Hint: You should be able to hit an error rate on the validation set that's lower than 2%.\n",
        "\n",
        "Note that this last exercise is *extremely simple*. All you need to do is a) create a SimpleMLP `Module` and instantiate that instead of `SimpleLinear`. The rest of your code should require nearly no changes at all (except for e.g. varying the learning rate)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Kb4UCStV2-xj",
        "colab": {}
      },
      "source": [
        "class SimpleMLP(torch.nn.Module):\n",
        "  \"\"\"A simple MLP model.\"\"\"\n",
        "  \n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    raise NotImplementedError\n",
        "  \n",
        "  def forward(self, x):\n",
        "    raise NotImplementedError\n",
        "    return x\n",
        "  \n",
        "model = SimpleMLP()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4e2vBtHbKRr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OG3_-ar36KMV",
        "colab": {}
      },
      "source": [
        "for module in model.children():\n",
        "  module.reset_parameters()\n",
        "\n",
        "info = []\n",
        "fig, ax = plt.subplots(2, 1, sharex=True)\n",
        "num_steps = 5000\n",
        "num_steps_per_val = 50\n",
        "best_val_err = 1.0\n",
        "for step in range(num_steps):\n",
        "  train_loss, train_err = train_step()\n",
        "  if step % num_steps_per_val == 0:\n",
        "    val_loss, val_err = val()\n",
        "    if val_err < best_val_err:\n",
        "      best_val_err = val_err\n",
        "      print('Step {:5d}: Obtained a best validation error of {:.3f}.'.format(step, best_val_err))\n",
        "    info.append([step, train_loss, val_loss, train_err, val_err])\n",
        "    x, y11, y12, y21, y22 = zip(*info)\n",
        "    ax[0].plot(x, y11, x, y12)\n",
        "    ax[0].legend(['Train loss', 'Val loss'])\n",
        "    ax[1].plot(x, y21, x, y22)\n",
        "    ax[1].legend(['Train err', 'Val err'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAjqXY7sbFWR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}